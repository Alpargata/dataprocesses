---
title: "FinalProject - Group7"
author: "Alma Parias, Rafael Muñoz, Beatrice Olivari, Omar Echbiki and Miguel Pérez "
date: "19/12/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(plyr)
library(ggplot2)
library(dplyr)
library(readr)
library(corrplot)
library(car)
library(FactoMineR)
library(RiverLoad)
library(GGally)
library(plotly)
library(Hmisc)
library(tidyr)
library(rworldmap)
library(rworldxtra)
library(caret)
library(lattice)
```

This is our final projct for the subject Data Processes. The topic we chose is **Climate Change**.  

## Abstract
Climate change and its impact on the earth have been a hot issue in the last years. Many scientific researches have been carried out in order to both analyse the phenomenon and try to find a realistic solution. Recently, after becoming aware that it is now to late to find a complete solution, the scientists have been focusing more on predicting the speed of the process in order to be prepared to face the problems. Our aim in this project is to try to predict how the rising of co2 in the last years will affect the state of glaciers in the next months. In order to make this prediction we investigated three different datasets and we processed the data in them with the goal to merge them together in a unique dataset to use for our analysis. Afterwards, we performed some machine learning techniques to make our prediction.

### Introduction
We would like to answer this question because Climate Change is a really important issue we have now. The poles are melting, the sea level is growing and the temperature is rising. All of these problems can change the world and the way we live. We think people must be aware of this and try to slow the process. 

### Related Work
[Participants in actions against climate change](https://climateaction.unfccc.int/) In this link can be found a map, where you can select a country and see the initiatives. This is done by the United Nations.   

[World Climate Summit Madrid](https://www.worldclimatesummit.org/) This year the world Climate Summit has been held in Madrid. Some of the topics were e-mobility, smart cities, sustinable forest and responsible resource extraction. This is a way to concienciate people and think new regulations and ideas.

[Study about the need of more studies to show that Climate Chage is real](https://www.nature.com/articles/s41558-018-0360-1) This study says that some people doesn't believe in empirical data and that more studies that shows the effects are needed.

[Lichens are a biosensor of climate change](https://www.mdpi.com/1424-2818/11/3/42) The lichens in Antarctica are a natural biosensor to measure the evolution of climate change. 

[Food distribution related with climate change](https://esajournals.onlinelibrary.wiley.com/doi/full/10.1002/ecs2.2645) Trophic interactions within food webs affect species distributions, coexistence, and provision of ecosystem services but can be strongly impacted by climatic changes.

 For further reading, here are [Some articles about how climate change is affecting the nature](https://www.nature.com/nclimate/research) Some examples about how climate change is affecting the nature and sme examples about why the climate is getting worse. 

## Exploratory Data Analysis
In this part the three datasets that we are going to use are going to be presented.  

### CO2 emissions
This dataset describes Trends in Atmospheric Carbon Dioxide and it contains data from **1958** to  **2018**. [Source](https://datahub.io/anuveyatsu/co2-ppm):
The raw dataset has the following features:

- **Date:** the day of the recording in *yy-mm-YYYY* format
- **Average [Parts per million]:** The monthly mean CO2 mole fraction determined from daily averages. If there are missing days concentrated either early or late in the month, the monthly mean is corrected to the middle of the month using the average seasonal cycle. Missing months are denoted by _-99.99_.
- **Interpolated [Parts per million]: ** Values from the average column and interpolated values where data are missing. Interpolated values are computed in two steps. First, we compute for each month the average seasonal cycle in a 7-year window around each monthly value. In this way the seasonal cycle is allowed to change slowly over time. We then determine the trend value for each month by removing the seasonal cycle; this result is shown in the trend column. Trend values are linearly interpolated for missing months. The interpolated monthly mean is then the sum of the average seasonal cycle value and the trend value for the missing month.
- **Trend [Parts per million]:** Seasonally corrected.
- **Number.of.Days:** Number of days for the recording: **-1** denotes no data for number of daily averages in the month.

```{r echo=F}
# Reading the data from file
co2data<-read.csv("data/co2.csv", header=TRUE, na.strings = TRUE )

#Creating a column for Year to preserve Date
co2data$Year = substr(co2data$Date, start = 0, stop = 4)
co2data$Year = factor(co2data$Year)
co2data$Date =  as.Date(co2data$Date)
co2data <- subset( co2data, select = -Decimal.Date ) #removing redondant information about date since there are only 7 observation we can remove them as they don't incide on the dataset 

outlier_values <- boxplot.stats(co2data$Average)$out

co2data <- co2data %>% 
  filter(Average != '-99.99') 
#variables correlation and feature engineering 
co2data.cor = cor(x = co2data$Average , y = co2data$Trend) # To deternine if 2 coloumns describe the same event
co2data.cor = cor(x = co2data$Average , y = co2data$Interpolated)
```

```
{r CO2 data processing, include=FALSE}
```
```{r echo=F}
summary(co2data)
```
#### Data Preparation 
Before proceeding to use the dataset we have to do some preliminary operation:

1. Removing non useful features such as _Decimal.Date_ since it is a repetition of Date

2. Converting the Date fields to R Date to be used in the plot, since ggplot2 treats it as a time series and handled differently

3. Removing the outliers, since in Average there are a few observations that have a value of **-99.99** this changes radically the scale of the plot
```{r }
outlier_values <- boxplot.stats(co2data$Average)$out
outlier_values
```
So we simply removed them, since those observation are only 7, and compared to the size of the dataset we can remove those specific observations.

We also wanted to know which was (in terms of climate change) the 'worst year' for our planet? (more polluted year, higher temperature...).

Premise: By worst year here we mean that we are looking for the highest CO2 level in the data 
_We expect that since the trend of the CO2 level is increasing year by year that the year with the highest CO2 PPM will be the last recorded one_
To answer this question it is necessary that we do more Data Manipulation using R package dplyr.
In order to answer the question we need to retrieve the data for the year to do so we did the following steps:

1. Create a new Data frame called _co2data_year_  using the Date field and taking only the Year
2. Transforming it into a Date field
3. Group by Year 
4. Setting the variable Trend to be the Mean for that year using the summarize function from dplyr



After doing so we get the following data frame:

```{r}
co2data_year <-  co2data %>% 
  group_by(Year) %>%
  summarise(Trend = median(Trend))

head(co2data_year)

```

Now we can plot the results.

To answer this question it is enough to run this simple query using again the dplyr package.
This command will return one value which is the date of the highest CO2 level

```{r}
worst_year <-co2data %>% 
  filter(Trend == max(Trend))  %>% 
  select(Date)
worst_year

```



Let's see a summary of the data: 



```{r echo=F}
co2data_year <-  co2data %>% 
  group_by(Year) %>%
  summarise(Trend = median(Trend))

co2data_year$Year = as.Date(co2data_year$Year, format("%Y"))

ggplot(data = co2data_year, aes(x = Year, y = Trend) )+
  geom_line(color = "#00AFBB", size = 1)+
  ylab("CO2 Level")+
  ggtitle("CO2 mean by Year") +
  theme(plot.title = element_text(hjust = 0.5))
```



### International comprehensive Ocean-Atmosphere (ICOADS-NOAA)
Dataset description
https://rda.ucar.edu/datasets/ds548.0/

The International Comprehensive Ocean-Atmosphere Data Set (ICOADS) is a global ocean marine meteorological and surface ocean dataset. It contains data measurements and visual observation collected by ships, moored and drifting buoys, coastal stations, and other marine and near-surface ocean platforms.  
Each marine report contains individual observations of meteorological and oceanographic variables, such as sea surface and air temperatures, wind, pressure, humidity, and cloudiness. The coverage is global and sampling density varies depending on date and geographic position relative to shipping routes and ocean observing systems. 
This dataset was download from GoogleBigQuery, because of its size, thaat' the reason because we only have data from 2010. 
The dataset consists of 76 columns and 100,000 rows, but many of the columns present a lot of missing values (in many cases more than a half of the observations), so that we had to drop many of them. 

#### Data processing
Taking into account the general topic of interest, we decided to consider only the features regarding temperature, pressure of water, weather, clouds and waves. 
However, after analysing the number of NA values into the columns, we realized that most of them weren’t useful due to the high percentage of missing values (sometimes more than a half) so we deleted them. 
After doing this process, we also deleted the remaining NA values, by getting rid of the rows containing them. Eventually, the final dataset was composed by 9 columns and 69,050 rows.

To understand the relationship between the variables, we proceeded with the study of the correlations between the variables. By plotting the correlation matrix, we could easily see that, as expected, the variables ```sea_level_temp``` and ```air_temperature``` were highly correlated. 

```{r echo=F}
ocean <- read.csv('data/icoads_noaa.csv', header = TRUE)


# Choosing the important features

# It could be interesting to analyse also present weather, waves and clouds but more than
# a half of their values are NA, so we decided to drop them. Moreover we drop the year since its the 
# same (2010) for every observation. We don't take the column country_code since most of
# the cells are empty.
newCols <- c("month", "day", "latitude", "longitude", "air_temperature", "sea_surface_temp", "sea_level_pressure")
newOcean <- ocean[newCols]

# Features engineering and Pearson correlation

# Omitting all the rows with NA values
newOcean <- na.omit(newOcean, seq_along(sea_surface_temp, air_temperature, sea_level_pressure))

# Correlation matrix to see the relationship between the variables
newOcean.cor = cor(newOcean)
```

Another observation which could be seen is that there is a negative correlation between ```sea_surface_temp``` and ```latitude```. To investigate more about this, plotted ```sea_surface_temp``` vs ```latitude``` and we could observ a pattern which, as expected, showed that temperature is low in the poles and increases the closer we get to the equator.

```{r}
plot(newOcean$latitude, newOcean$sea_surface_temp)
```

Afterwards, in order to make the manipulation easier, we aggregated month and day into the variable Date. Moreover, we created a new variable, called Hemisphere, by binning the latitude.

```{r echo=F}

# we aggregate the date to easen the manipulation
newOcean$Date <- with(newOcean, sprintf("%d-%02d", month, day))

#Creating variable Hemisphere by binning the latitude
newOcean$Hemisphere <- cut(newOcean$latitude, c(-90,0, 90))
newOcean$Hemisphere <- revalue(newOcean$Hemisphere, c("(-90,0]"="South"))
newOcean$Hemisphere <- revalue(newOcean$Hemisphere, c("(0,90]"="North"))
```

#### Graphs
1.  Firstly, we choose to represent the values of the ```sea_level_pressure``` in a world map. Lower values of pressure are represented by blue colours, while higher values are represented by yellow and red colours.


2.  In order to make a comparison between the evolution of temperature during the year in the northern and southern pole, we plotted two bar charts.

```{r echo=F}
# Plotting sea_level_pressure
layout(matrix(1:2,ncol=2), width = c(2,1),height = c(1,1))

newmap <- getMap(resolution = "high")
pal = colorRampPalette(c("blue","lightblue", "yellow", "red"))
newOcean$order = findInterval(newOcean$sea_level_pressure, sort(newOcean$sea_level_pressure))
plot(newmap, xlim = c(-20, 59), ylim = c(35, 71), asp = 1, col = "antiquewhite1")
points(newOcean$longitude, newOcean$latitude, col=pal(nrow(newOcean))[newOcean$order], pch = 16, cex = .6)

colfunc <- colorRampPalette(c("red","yellow", "lightblue", "blue"))
legend_image <- as.raster(matrix(colfunc(20), ncol=1))
plot(c(0,2),c(0,1),type = 'n', axes = F,xlab = '', ylab = '', main = 'Pressure in hPa')
text(x=0.8, y = seq(0,1,l=2), labels = seq(955.5,1046.6,l=2))
rasterImage(legend_image, 0, 0, 0.5,0.5)

#Avarage temperature by months for the different hemispheres
ocean_south <- newOcean %>% subset(Hemisphere == "South")
ocean_south <- aggregate(ocean_south,
                         by = list(ocean_south$month),
                         FUN = mean)
cols <- c("month", "air_temperature", "sea_surface_temp", "sea_level_pressure")
ocean_south <- ocean_south[cols]

ocean_north <- newOcean %>% subset(Hemisphere == "North")
ocean_north <- aggregate(ocean_north,
                         by = list(ocean_north$month),
                         FUN = mean)
ocean_north <- ocean_north[cols]

ggplot(data=ocean_north, aes(x=month, y=air_temperature,2)) +
  scale_x_discrete(limits=c("1", "2", "3", "4", "5", "6", "7", "8", "9", "10", "11", "12"))+
  theme(axis.text.x = element_text(size=10, angle=45))+
  ggtitle("Sea level pressure by month in the Northern Emisphere")+
  geom_bar(stat="identity", fill="steelblue")+
  theme_minimal()
```
```{r echo=F}

ggplot(data=ocean_south, aes(x=month, y=air_temperature,2)) +
  scale_x_discrete(limits=c("1", "2", "3", "4", "5", "6", "7", "8", "9", "10", "11", "12"))+
  theme(axis.text.x = element_text(size=10, angle=45))+
  ggtitle("Sea level pressure by month in the Southern Emisphere")+
  geom_bar(stat="identity", fill="steelblue")+
  theme_minimal()

```

### The National Snow and Ice Data Center
The National Snow and Ice Data Center (NSIDC) supports research into our world’s frozen realms: the snow, ice, glaciers, frozen ground, and climate interactions that make up Earth’s cryosphere. NSIDC manages and distributes scientific data, creates tools for data access, supports data users, performs scientific research, and educates the public about the cryosphere.

#### Content
The dataset provides the total extent for each day for the entire time period (1978-2015). There are seven variables: 

 - Year
 - Month
 - Day
 - Extent: unit is 10^6 sq km
 - Missing: unit is 10^6 sq km
 - Source: Source data product web site: http://nsidc.org/data/nsidc-0051.html
 - Hemisphere: North or South
 
 
The main problem with this dataset it that measures are taken every 2 days, while the co2 are monthly. 

```{r, echo=F}
data<-read.csv('data/seaice.csv',  header=TRUE)
summary(data)
```

For this reason, we had to:
 - Take a look at the possible exising cycles of ice extent during each month
 - See if we can compress all that information in a few features (like mean and variance or jitter or some statistical feature). 
 
```{r echo=F}
# Data Preprocess
df <- subset( data, select = -Source.Data ) #We remove the link provided, since its
#not useful for our purpose at least in first place
df$Date=as.Date(paste(sep="-",df$Year,df$Month,df$Day)) # we aggregate the date to easen the
#manipulation
ag<-aggregate(df['Extent'], by=df['Date'], sum)
names(ag)[names(ag)=="Extent"]<-"Global Extent"
rs<-aggregate(x = df["Extent"], by=df["Date"], 
              FUN = function(a){
                y<-a[1]-a[2];
              })
names(rs)[names(rs)=="Extent"]<-"North-South Difference"

df <- merge(df,ag,by="Date")
df <- merge(df,rs,by="Date")

north = df[df$hemisphere == "north",]
south = df[df$hemisphere == "south",]
# Statistical exploration in data

lm1 = lm(data=df, Extent~ . )
anova(lm1)

```
 
Thankfully, we saw almost no variation within the days, as was seen on ```IceSurface.R```, the ice extent remains almost constant for each day of the month. This is reasonable, since it has no sense having a sensible decrease in ice extent in a certain days, since it is a rather long-termed effect.

```{r echo=F, warning=F}
anAvg<-aggregate(df['Global Extent'], by=df['Year'], sum)
anAvg$`Global Average`<-anAvg$`Global Extent`/table(df$Year)

ggplot(anAvg,aes(x =Year, y =`Global Average` ))+
  geom_point() + 
  geom_smooth(method='lm', formula= y~x)
```


#### Questions of interest about Ice
The other thing we did is we provided a different columns for each one of the hemisphere, instead of having 2 rows for each observation:  one for northern hemisphere and other for the southern, and added two extra columns: global extent and difference between north and south. This can be done, since all measures are an absolute value (10^6 km squared) and they are additive.

#Question 1: How was the ice surface's evolution along all the time?
As we can see, both hemisphere's extention on ice follow a cycle per year in which they compensate each other.
However, we must note that the south hemisphere suffers a way higher variations in the extent than the northern hemisphere.
We can infer that these cycles correspond to the 6 month season that take place on each pole, since they have 
lower values during aproximately 6 months and then oscilate to a 6 months period of max values.

We can also see here the corresponding values for each hemisphere.
As we can see, the south one is almost constant along the years.
But the north one is significatively damaged, as we can see from the regression line.

#Question 2: Is there a cycle of ice regarding the days of the month?

As we can see, there is no obious cycle in the days, maybe a little differences in the last days, but not a great difference. 

#Question 3: Is there a cycle of ice regarding months of the year? Maybe related to seasonal weather?

We can see that the seasonal effect is strong. However, for many of the months there are a lot of outliers, so it may imply that many exceptions to the normal value occurred. Maybe due to the trend along all years?

#Question 4: Is there a different amount of extention of ice between the North
Hemisphere and the South Hemisphere?

We noted that before, there is.

#Question 5: Annual average of ice?
we can see that the average annual amount of ice is sharply decreasing: 2.5 M km^2 lost in less than 50 years!

 
## Methods
To be able to create a model, first we need to have one dataset. In this case, we are only going to use **The  National Snow and Ice Data Center**  and **CO2**. 

We merged all the data via month, and calculated the mean for each month. After that, we proceeded to merge co2 dataset and ice extent dataset. 
This was an easy task after rearranging the times, but we were careful in the merging, so missing values are completely eliminated from both datasets. 

After doing that, we explored the data. We could observe that ice variation in the South pole was almost insignificant, since the regression line was straight completely and variation remained the same. The main concern was for the north pole extent, which is greatly damaged by the co2. We made a graph representing it that can be seen below.


### Strength of relationships

The relationships between Co2 and Ice extension were tested.

```{r echo=F}
data<-read.csv("data/MergedDataset.csv",  header=TRUE)

ggcorr(data)
r=cov(data[,c(2,6)]) # We have here the covariance matrix between North and CO2.
rcor=rcorr(as.matrix(data[,c(2,6)])) # Here the correlation matrix
corrplot(rcor$r) # here we can see that there exists a negative relation between co2 and Ice surface

```
This plots show us an existing and negative correlation between the North hemisphere ice extension and CO2.
This means that if CO2 concentrations increase, north hemisphere ice will decrease. Correlation and partial correlation were tested.
Correlation between CO2 and North Ice Extension was -0.21. This may lead to confounding conclusions, since the value is very low (in the 0 closeness). Thus, we also performed a Pearson partial correlation and obtained that the partial correlation was -0.447, so if all other values are controlled to not interfere in the correlation between these 2 variables. So in this case, the relationship is higher, meaining that even the relationship between the 2 variables is not linear, it exist a negative relationship between both of them.

```{r}
fit <- lm(NorthIceAverageExtent ~ Trend.for.Co2.amount ,data = data)

scatterplot <- data %>%
  ggplot(aes(x = Trend.for.Co2.amount, y = NorthIceAverageExtent, color = Date)) + 
  ggtitle("Correlation between North Hemisphere Ice surface and CO2 concentration")+
  theme(plot.title = element_text(hjust = 0.5))+
  geom_point() + 
  labs(x = "Co2 Amounts (mole fraction)", 
       y = "Ice extent (10^6 km)") +
  theme(legend.position = "none")

ggplotly(scatterplot) %>% 
  add_lines(x = ~data$Trend.for.Co2.amount, y = fitted(fit))
```

### Predictions
We want to predict the Extent in sea ice based on the amount of CO2 (Trend) present in the atmosphere. To do that, we need to merge the information from two datasets: co2 and seaice.

We are going to join them by the Date, but before doing it, we need to clean and select the columns (make sure that there is no null values), aggregate the information by year and month in seaice (because in the co2 dataset the data is aggregated by month) and create a new Date variable from these. 
Then, we perform an inner join to make sure that we will not have missing values. 

```{r}
#Load co2_data, select colums and format Date
co2_data<-read.csv("../data/co2.csv",header=TRUE, na.strings = TRUE , as.is = TRUE)
co2_data <- co2_data %>% select(-Decimal.Date) %>% select(-Number.of.Days)
co2_data$Date<-as.Date(co2_data$Date, format = "%Y-%m-%d")
print(paste0('number of missing values in the co2 dataset: ',sum(is.na(co2_data))))

#Load ice_data and select columns
ice_data<-read.csv("../data/seaice.csv",header=TRUE)
ice_data<-ice_data %>% select(-Source.Data) %>% select (-Missing) %>% select (-Day)
#Grouping by year and month, sort and format date
ice_data<-ice_data %>% group_by(Year, Month) %>% summarise(Extent=mean(Extent)) %>% arrange(Year, Month)
ice_data$Date<-as.Date(paste0(ice_data$Year, "-", ice_data$Month,'-01'), format = "%Y-%m-%d")
print(paste0('number of missing values in the ice dataset: ',sum(is.na(co2_data))))

#Join both datasets by Date field
joined_df<-inner_join(ice_data, co2_data, by='Date')
```

After joining the datasets, we perform a linear model to observe how well our model fits into a line: ```Extent ~ Trend + Year + Month```. 
The Multiple R-squared is telling us that this mode explains 64.5% of the variance present in the target variable. 

```{r echo=F}
linear_model<-lm(Extent ~ Trend + Year + Month, data = joined_df)
summary(linear_model)
```

But if we convert the Month to categorical variable (using as.factor function), we get a Multiple R-squared of 0.9438; a huge improvement. This is telling us that the value of Extent depends heavily on the month of the year.

```{r echo=F}
linear_model_categorical<-lm(Extent ~ Trend + Year + as.factor(Month), data = joined_df)
summary(linear_model_categorical)
```

After that, it is time to start applying machine learning algorithms using the caret package. We have chosen two: Knn and random forest. The first one is simple and powerful, and the second one is easy to understand and is good at handling both categorical and numerical data. 

```{r}
modeling_df<-joined_df %>% select(-Date) %>% select(-Average) %>% select(-Interpolated)
trainIndex <- createDataPartition(modeling_df$Extent,
                                  p = .8,       # Proportion of data used for training
                                  list = FALSE, # Return the values as a vector (as opposed to a list)
                                  times = 1     # Only create one set of training indices
)
```

After randomly splitting the dataset into training (80%) and testing (20%), Knn is faster at training and achieves an RMSE around 0.6. 

```{r}
# Subset your data into training and testing set
training_set <- modeling_df[ trainIndex, ] # Select rows with generated indices
test_set <- modeling_df[ -trainIndex, ]    # Remove rows with generated indices

# Grid for the hyperparameter tunning
rf_grid <- expand.grid(mtry=c(3,6,9,12,15))
knn_grid <- expand.grid(k=c(3,4,5,6,7))
knn_model<- train(Extent ~ Trend + Year + Month, data = training_set, 
                  method = "knn", trControl=trainControl(method = "cv", number=3), tuneGrid=knn_grid)
rf_model<- train(Extent ~ Trend + Year + as.factor(Month), data = training_set, 
                 method = "cforest", trControl=trainControl(method = "cv", number=3), tuneGrid=rf_grid)

```

Comparing to the standard deviation of the target variable (1.43) is not bad, but still improvable. 
We decided to use a **random forest** and include the variable ```Month``` as categorical. 
It takes longer to train, but we get an RMSE around 0.34, which is much better. Training is performed with cross-validation technique (k=3) and parameter tuning is done with grid search.

```{r}
# Compute statistics on the target variable
summary(modeling_df$Extent)
sd(modeling_df$Extent)

# Compute the prediction error RMSE
print(paste0('RMSE of prediction with knn model in training dataset: ', RMSE(predict(knn_model, training_set), training_set$Extent)))
print(paste0('RMSE of prediction with knn model in test dataset: ', RMSE(predict(knn_model, test_set), test_set$Extent)))

# Compute the prediction error RMSE
print(paste0('RMSE of prediction with random forest model in training dataset: ', RMSE(predict(rf_model, training_set), training_set$Extent)))
print(paste0('RMSE of prediction with random forest model in test dataset: ', RMSE(predict(rf_model, test_set), test_set$Extent)))

```

We can conclude, after the linear model and the random forest, that we can find a clear pattern in the data from these two datasets; meaning that we can predict the extent of ice through co2 level, year and month of the year.



## Results

## Discussion and future work  
Another interesting approach would have been to use time series analysis. There are interesting aspects in the data that could be understan from the perspective of the time: seasonality, trend, etc. 
We would have also liked to include data from the ICOADS dataset(oceans) but, unfortunately, we only had data from 2010. 
The ICOADS dataset is also temporal, but it is huge. The dataset is in GoogleBigQueries, so we can't download complete. As future work it will be interesting using RSpark and use the whole dataset to try to improve the predictions with the data af the oceans. 



```{r echo=F}
```
